![image](https://img-blog.csdnimg.cn/20181221201205769.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhbzE5OTQxMjE=,size_16,color_FFFFFF,t_70)

## 一、drop out
1. 每个神经元以概率p被丢弃
2. 未丢弃的神经元的输入要除以(1-p)，作为神经元失活的补偿，以保证本层的drop out不影响下一层

   ```python
   def dropout(x, p):
      if p < 0. or p >= 1:# p是概率值，必须在0~1之间
         raise Exception('Dropout p must be in interval [0, 1]')
      retain_prob = 1 - p
      #我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样
      #硬币 正面的概率为p，n表示每个神经元试验的次数
      #因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。
      sample=np.random.binomial(n=1,p=retain_prob,size=x.shape)#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了
      print sample
      x *=sample # 0、1与x相乘，我们就可以屏蔽某些神经元，让它们的值变为0
      print x
      x /= retain_prob # 使得没有被丢弃的神经元的值增大
   
      return x
   ```

## 二、Batch Normalization
1. **什么是BN？**  
   
   每次SGD时，通过**mini-batch的均值和方差**对相应的activation做规范化，使得每一个**特征维度**分别归一化。再通过“scale and shift”**重构原始分布**，以避免影响模型的表达能力。
    <div align=center>
        <p><img src="https://pic1.zhimg.com/80/9ad70be49c408d464c71b8e9a006d141_1440w.jpg" width="50%" height="50%">
        <p>
        <p>目标是让每个神经元在训练过程中学习对应的两个调节因子。也就是说，<img src="https://www.zhihu.com/equation?tex=%5Cgamma%5E%7B%28k%29%7D%3D%5Csqrt%7BVar%5Bx%5E%7B%28k%29%7D%5D%7D%2C+%5Cbeta%5E%7B%28k%29%7D%3DE%5Bx%5E%7B%28k%29%7D%5D" width="30%" height="30%">是可学习的参数，都是长度等于特征维度的向量，满足公式条件时，相当于batch normalization没起作用，恢复原始分布。
    </div>  

2. **BN参数如何学习？**  
   
   <div align=center>
        <p><img src="https://pic1.zhimg.com/80/beb44145200caafe24fe88e7480e9730_1440w.jpg" width="50%" height="50%">
   </div>
3. **BN用在哪里？**  
   
   BN用在激活单元。  
   
   论文指出在CNN中，BN应用在非线性映射前，即对$z=Wx+b$做规范化。
4. **为什么BN？**  
   
   Internal Covariate Shift(ICS, 内部协变量偏移)是google提出BN所解决的问题，即神经网络各层的输入的数据分布在参数更新过程中发生了变化。covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即对所有$x\in\chi, P_s(Y|X=x)=P_t(Y|X=x)$，$P_s(X){\ne}P_t(X)$。的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。  

   但是通过归一化后重构的方式解决上述问题是微乎其微的，BN的主要作用是防止梯度弥散（通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大）和梯度爆炸，以及加快训练速度，提高模型精度。  

   PS: 研究表明，BN真正的用处在于**平滑损失平面**：通过上文所述的 Normalization 操作，使得网络参数重整（Reparametrize），它对于非线性非凸问题复杂的损失曲面有很好的平滑作用，参数重整后的损失曲面比未重整前的参数损失曲面平滑许多。此外，也使得学习率可以设定为较大的值，加快模型训练的速度。

5. **BN如何防止梯度弥散？**
   
   - 不使用BN时，连续多层的梯度反向传播过程为：  
      $$
      \begin{aligned}
         \frac{\partial H_l}{\partial H_k} 
         &= {\frac{\partial H_l}{\partial H_{l-1}} \frac{\partial H_{l-1}}{\partial H_{l-2}}...\frac{\partial H_{k+1}}{\partial H_k}}\\ 
         &= {\frac{\partial ({W_l}^TH_{l-1})}{\partial H_{l-1}} \frac{\partial ({W_{l-1}}^TH_{l-2})}{\partial H_{l-2}}...\frac{\partial ({W_{k+1}}^TH_{k})}{\partial H_k}}\\ 
         &= \prod_{i=k+1}^lW_i\\
      \end{aligned}
      $$
   - 使用BN后，连续多层的梯度反向传播过程为（其中$\mu_l$为列向量，$\frac{1}{\sigma_l}$为$diag(\frac{1}{\sigma_{l_1}},\frac{1}{\sigma_{l_2}},...,\frac{1}{\sigma_{l_n}})$）：
      $$
      \begin{aligned}
         \frac{\partial H_l}{\partial H_k} 
         &= {\frac{\partial H_l}{\partial H_{l-1}} \frac{\partial H_{l-1}}{\partial H_{l-2}}...\frac{\partial H_{k+1}}{\partial H_k}}\\ 
         &= {\frac{\partial BN({W_l}^TH_{l-1})}{\partial H_{l-1}} \frac{\partial BN({W_{l-1}}^TH_{l-2})}{\partial H_{l-2}}...\frac{\partial BN({W_{k+1}}^TH_{k})}{\partial H_k}}\\ 
         &= {\frac{\partial (\frac{{W_l}^TH_{l-1}}{\sigma_l}-\mu_l)}{\partial H_{l-1}} \frac{\partial (\frac{{W_{l-1}}^TH_{l-2}}{\sigma_{l-1}}-\mu_{l-1})}{\partial H_{l-2}}...\frac{\partial (\frac{{W_{k+1}}^TH_k}{\sigma_{k+1}}-\mu_{k+1})}{\partial H_k}}\\ 
         &= [\frac{\partial (\frac{{W_l}^TH_{l-1}}{\sigma_l}-\mu_l)}{\partial {W_l}^TH_{l-1}}\frac{\partial {W_l}^TH_{l-1}}{\partial H_{l-1}}][ \frac{\partial (\frac{{W_{l-1}}^TH_{l-2}}{\sigma_{l-1}}-\mu_{l-1})}{\partial {W_{l-1}}^TH_{l-2}}\frac{\partial {W_{l-1}}^TH_{l-2}}{\partial H_{l-2}}]...[\frac{\partial (\frac{{W_{k+1}}^TH_k}{\sigma_{k+1}}-\mu_{k+1})}{\partial {W_{k+1}}^TH_k}\frac{\partial {W_{k+1}}^TH_k}{\partial H_k}]\\ 
         &= (W_l\frac{1}{\sigma_l})(W_{l-1}\frac{1}{\sigma_{l-1}})...(W_{k+1}\frac{1}{\sigma_{k+1}}) \\
         &= \prod_{i=k+1}^lW_i \frac{1}{\sigma_i}\\
      \end{aligned}
      $$
      
   可以看出，与不使用BN相比，每层的反向传播过程中，增加了一个基于标准差的矩阵$\frac{1}{\sigma_{l_i}}$对权重$W_i$进行放缩。  

   **这样的缩放能够产生什么效果？** 方法让我们分析一下，如果权重$W_i$较小，那必然${W_i}^TH_{i-1}$较小，从而使得其标准差$\sigma_i$较小，相对的$\frac{1}{\sigma_{l_i}}$较大，所以$W_i\frac{1}{\sigma_{l_i}}$相对于原本的$W_i$就放大了，避免了梯度的衰减；同样的，如果权重$W_i$较大，可以很容易得到$W_i\frac{1}{\sigma_{l_i}}$相对于原本的$W_i$缩小了，避免了梯度的膨胀。于是，加入了BN的反向传播过程中，就不易出现梯度消失或梯度爆炸，梯度将始终保持在一个合理的范围内。而这样带来的好处就是，基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题。

6. 训练时和推理时统计量不一致。模型预测时，BN使用的是移动平均值。测试时：都是对单个样本进行测试，就不存咋batch的概念。这个时候的均值和方差是全部训练数据的均值和方差，这两个数值是通过移动平均法求得。BN层在训练阶段，对每一批mini-batch的数据分别求均值与方差用于规范化，但在测试阶段，这一值使用的是全期训练数据的均值方差，也即模型训练到当前状态所有数据的均值方差，这个数据是在训练过程中通过移动平均法得到的。综合来看，BN层在训练和测试的主要区别在于：训练时的均值方差来源于当前的mini-batch数据，而测试时，则要使用训练使用过的全部数据的均值方差，这一点训练时就通过移动均值方法计算并保存下来了；Dropout方法的训练测试区别在于：训练时随机的关掉部分神经元，而测试时所有神经元都工作但都要乘上系数（可以理解为训练了很多子模型，测试时要求他们加权求和的结果，听起来更像bagging了，emmm）

7. BN对batch size有依赖，当batch size较大时，有不错的效果。而LN、IN、GN能够摆脱这种依赖，其中GN效果最好。
   <div align=center>
   <img src="https://img-blog.csdnimg.cn/20181221201748697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhbzE5OTQxMjE=,size_16,color_FFFFFF,t_70" width="50%" height="50%">
   </div>
8. BN对较小的batch size效果不好。BN适用于固定深度的前向神经网络，如CNN，不适用于RNN；对于RNN等时序模型，RNN 的每个时间步需要维护各自的统计量，而 Mini-Batch中的训练实例长短不一(不同长度的句子)，这意味不同时间步的隐层会看到不同数量的输入数据，而这会给BN的正确使用带来问题，只能使用Layer Normalization；
9. LN对RNN效果明显；LN的一个优势在于不需要批训练，在单条数据内部就能归一化。LN不依赖于batch size和sequence length，因此可以用于batch size为1和RNN中。  
10. IN用在风格化迁移；作者发现，在生成模型中， feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格，因此可以先把图像在 channel 层面归一化，然后再用目标风格图片对应 channel 的均值和标准差“去归一化”，以期获得目标图片的风格。
11. Group Normalization (GN) 适用于占用显存比较大的任务，例如图像分割。对这类任务，可能 batchsize 只能是个位数，再大显存就不够用了。而当 batchsize 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。



至于深度学习中的 Normalization，因为神经网络里主要有两类实体：神经元或者连接神经元的边，所以按照规范化操作涉及对象的不同可以分为两大类，一类是对第 L 层每个神经元的激活值或者说对于第 L+1 层网络神经元的输入值进行 Normalization 操作，比如 BatchNorm/LayerNorm/InstanceNorm/GroupNorm 等方法都属于这一类；另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，比如 Weight Norm 就属于这一类。广义上讲，一般机器学习里看到的损失函数里面加入的对参数的的 L1/L2 等正则项，本质上也属于这第二类规范化操作。L1 正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得 0 值的效果，而 L2 正则的规范化目标是有效减小原始参数值的大小。有了这些规范目标，通过具体的规范化手段来改变参数值，以达到避免模型过拟合的目的。

   ```python
   class BN(object):
      def __init__(self,shape):
         self.moving_mean=np.zeros(shape[1])
         self.moving_val=np.zeros(shape[1])
         self.epsilon =0.000001
         self.moving_decay = 0.997
         self.batchsize = shape[0]
         pass

      def forward(self,x):
         self.mean = np.mean(x,axis=(0,2,3))
         #self.val = np.val(x,sxis=(0,2,3))
         self.val = self.bactchsize/(self.batchsize-1)*np.val(x,axis=(0,2,3)) if self.batchsize>1 else np.val(x,axis=(0,2,3))

         if np.sum(self.moving_mean) == 0 and np.sum(self.moving_val) == 0:
            self.moving_mean = sel.mean
            self.moving_val = sel.val


         self.moving_mean = sel.moving_decay * self.moving_mean + (1-self.moving_decay)*self.mean 
         self.moving_val = sel.moving_decay * self.moving_val + (1-self.moving_decay)*self.val

         if self.training:
            self.normal_x = (x-self.mean)/np.sqrt(self.val+self.epsilon)
         else:
            self.normal_x = (x-self.moving_mean)/np.sqrt(self.moving_val+self.epsilon)

         return self.normal_x*self.alpha + self.beta

      def gradient(self, eta):
         self.a_gradient = np.sum(eta * self.normed_x, axis=(0, 1, 2))
         self.b_gradient = np.sum(eta * self.normed_x, axis=(0, 1, 2))


         normed_x_gradient = eta * self.alpha
         var_gradient = np.sum(-1.0/2*normed_x_gradient*(self.input_data - self.mean)/(self.var+self.epsilon)**(3.0/2), axis=(0,1,2))
         mean_gradinet = np.sum(-1/np.sqrt(self.var+self.epsilon)*normed_x_gradient, axis=(0,1,2))

         x_gradient = normed_x_gradient*np.sqrt(self.var+self.epsilon)+2*(self.input_data-self.mean)*var_gradient/self.batch_size+mean_gradinet/self.batch_size

         return x_gradient

      def backward(self, alpha=0.0001):
         self.alpha -= alpha * self.a_gradient
         self.beta -= alpha * self.b_gradient

   ```

# 三、Layer Normalization  
1. 示意图  
   <div>
      <img src="https://pic1.zhimg.com/80/b710a578f73f478e3414d82e11c3055c_1440w.png" width="70%">
   </div>
- reference:  
[1] [机器之心|深度学习中的Normalization模型](https://www.jiqizhixin.com/articles/2018-08-29-7)
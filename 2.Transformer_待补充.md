<div align=center><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3d3MS5zaW5haW1nLmNuL2xhcmdlLzAwNmdPZWlTbHkxZzB6N3AwM204ZmozMGZsMGtybXlxLmpwZw?x-oss-process=image/format.png"><p><img src="https://pic4.zhimg.com/v2-9d17523075674db4006d20edbe5ec553_b.jpg" width="50%"></div>

- reference:   
[1] [The Annotated Transformer|配合代码看论文](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  
[2] [ppt](https://www.jianshu.com/p/79a50f085765)  
[3] [好文章](https://www.pianshen.com/article/1916976296/)（[原文](https://blog.csdn.net/Mr_green_bean/article/details/104816750)）  
[4] [详解Transformer （Attention Is All You Need）|知乎-刘岩](https://zhuanlan.zhihu.com/p/48508221)  
[5] [《Attention is All You Need》浅读（简介+代码）|苏剑林](https://kexue.fm/archives/4765)  
[6] [[整理] 聊聊 Transformer|知乎-简枫](https://zhuanlan.zhihu.com/p/47812375)  
[7] [宅家NLP (二) —— Self-Attention与Transformer|知乎-不会游泳](https://zhuanlan.zhihu.com/p/107501231)  




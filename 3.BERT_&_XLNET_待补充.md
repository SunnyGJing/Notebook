## XLNet
1. **自回归AR语言模型 vs. 自动编码器AE语言模型**
   - 介绍
     - 自回归：从左向右根据前文预测下一个单词或从右向左根据后文预测前一个单词
     - 自编码：从引入噪声的输入中重建原始数据
   - 优缺点分析
     - 自回归：不能同时获取单词的上下文信息
     - 自编码：预训练阶段（存在[Mask] 字符）和finetuning 阶段（无[Mask] 字符）文本分布不一致；token的独立性假设
   - 如图
        ![](https://pic2.zhimg.com/v2-ff31b4f099f3ee7767c07de3d405f339_b.jpg)

2. XLNet vs. BERT
    - BERT是显式地在输入句子中加入mask标记随机遮盖15%token，XLNET是隐式地在Attention Mask中实现同等效果，越靠前地单词被遮盖的概率越高。（以及query流自注意力机制的作用）
    
    - BERT是预测mask住的单词以及配合预测句子对，XLNET是预测当前位置token以及配合预测句尾的1/k个单词（为了调高效率）。
    
    - XLNet 计算量和激活值比 BERT 更大，有可能接近 BERT 的 2 倍（因为 XLNet 要给每个位置维护两套隐状态，一套是包含当前 token 信息的 content stream h，另一套是不包含当前 token 信息的 query stream g。此外还有前一个 segment 的缓存）。细节详见论文 2.3 节 Two-Stream Self-Attention 的相关描述
  
3. blabla
   - XLNet 的提升来源主要有两部分，一是来源于 Transformer-XL 对更长的上文信息的提取，二是来源于 PLM 训练目标与下游任务的一致性（没有 train-test skewness）。
   - 接下来的两个设计主要为了解决构建Permutation Language Model带来的两个问题：
        - 序列进行因式分解使得token失去位置感。
          - Reparameterization with positions：由于Permutation Language Model对句子顺序做了随机打乱，可能会导致模型失去对某个token在句子中的原始位置的感知，导致模型退化成一个词袋模型，于是作者利用Reparameterization with positions去解决这个问题。

        - 序列进行因式分解使得进行预测时模型能够提前看到答案。
          - Two-stream attention
            - 1.一个query stream只编码目标单词以外的上下文的信息以及目标单词的位置信息。   
            - 2.一个content stream既编码目标单词自己的信息，又编码上下文的信息供。  
            - query stream 的作用就是避免了模型在预训练时提前预知答案。 而做下游任务 fine-tune 时将query stream去掉，这样就完美的解决了这个问题。
4. 重排列语言模型
    ![](https://pic1.zhimg.com/v2-666ff26cdc26d6baaa5d54ec7bbf5c8c_b.jpg)
   - (New Target)
   - (Position Info) 但是在原来的公式中，我们只使用了$h_θ(x_(Z<t))$来表示当前token“上文”的hidden representation，使得不管模型要预测哪个位置的 token，如果“上文”一致，那么输出就是一致的。因此，新的公式做出了改变，引入了要预测的 token 的位置信息。**（这个公式看不懂？？？？？？？）**
   - (Partial Prediction)此外，为了降低模型的优化难度，XLNet 使用了 Partial Prediction，即只预测当前 permutation 位置 c 之后的 token，最终优化目标如Partial Prediction所示。

5. **双流自注意力机制**
   ![](https://pic4.zhimg.com/v2-39fb8b8f41f4ca079bb5fb1da743dfdb_b.jpg)
   - 内容流自注意力机制content stream
   - query流自注意力机制query stream

    - 该机制所要解决的问题是，当我们获得了$g_θ (x_{Z<t},z_t)$ 后，我们只有该位置信息以及“上文”的信息，不足以去预测该位置后的 token；而原来的 $h_θ (x_{Z<t})$ 则因为获取不到位置信息，依然不足以去预测。因此，XLNet 引入了 Two-Stream Self-Attention 机制，将两者结合起来。
6. Recurrence Mechanism
   ![](https://pic4.zhimg.com/v2-801414f441e2196a4e606508ce1cae4b_b.jpg)
   - 该机制来自 Transformer-XL，即在处理下一个 segment 时结合上个 segment 的 hidden representation，使得模型能够获得更长距离的上下文信息。而在 XLNet 中，虽然在前端采用相对位置编码，但在表示$h_θ (x_{Z<t})$的时候，涉及到的处理与permutation 独立，因此还可以沿用这个机制。该机制使得 XLNet 在处理长文档时具有较好的优势。

7. **我们上文提到过，XLNet起作用的，如果宏观归纳一下，共有三个因素**

   - 与Bert采取De-noising Autoencoder方式不同的新的预训练目标：Permutation Language Model(重排列语言模型，简称PLM)；维持了表面看上去的自回归LM的从左向右的模式，利用Transformer中的Attention mask随机遮盖句子中的单词，从而融入双向语言模型，在上文中添加了下文信息。这个是XLNet在模型角度比较大的贡献，确实也打开了NLP中两阶段模式潮流的一个新思路。

   - 引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制。实践已经证明这两点对于长文档任务是很有帮助的；

   - 加大增加了预训练阶段使用的数据规模；Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。这个明显走的是GPT2.0的路线。

8. 作者的总结ppt
    ![](https://pic4.zhimg.com/v2-bc72f59ec693d60a9adb7928b326e7f7_b.jpg)
9. BERT的任务领域
    - 输入不太长，最好是句子或者段落，避免Bert长文档的问题；

    - 语言本身就能够很好的解决问题，不依赖其它类型的特征；

    - 非生成类任务，避开目前Bert做生成类任务效果不够好的雷点；

    - 最好是能够涉及到多句子关系判断类任务，充分利用Bert 善于做句子匹配任务的特点；

    - 最好是能够牵扯到语义层级的任务，充分利用Bert能够编码深层语言知识的优点；

    - 如果是单输入问题，你想想能不能加入辅助句，把它改造成句子匹配型双输入任务；
10. 公平地比较 XLNet 与 BERT
    - 为了更好地说明 XLNet 的优越性，XLNet 团队发表了开头提到的博文“A Fair Comparison Study of XLNet and BERT”。
    
    - 在这篇博文中，XLNet 团队控制 XLNet 的训练数据、超参数（Hyperparameter）以及网格搜索空间（Grid Search Space）等与 BERT 一致，同时还给出了三个版本的 BERT 进行比较。BERT 一方则使用以下三个模型中表现最好的模型。

    ![](https://pic4.zhimg.com/v2-f1ba7c9bfd56b10838f8e3518a804dcf_b.jpg)
    ![](https://pic3.zhimg.com/v2-5c339a8103367eeb4bca71e57ee136a2_b.jpg)

    - 从中可以看出，在相同设定情况下，XLNet 完胜 BERT。但有趣的是：XLNet 在使用 Wikibooks 数据集时，在MRPC（Microsoft Research Paraphrase Corpus: 句子对来源于对同一条新闻的评论，判断这一对句子在语义上是否相同）和 QQP（Quora Question Pairs: 这是一个二分类数据集。目的是判断两个来自于 Quora 的问题句子在语义上是否是等价的）任务上获得了不弱于原版 XLNet 的表现；BERT-WWM 模型普遍表现都优于原 BERT；去掉 NSP（Next Sentence Prediction）的 BERT 在某些任务中表现会更好；


```
RoBERTa 对 BERT 的改进更是体现了这一点，
作者直接通过实验证明了 BERT 两大任务之一的 NSP 是几乎无效的，
然后通过调参、更大的数据集、更长的序列强行超越了 XLNet...
```